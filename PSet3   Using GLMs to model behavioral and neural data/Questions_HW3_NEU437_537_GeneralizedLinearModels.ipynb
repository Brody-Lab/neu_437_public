{"cells":[{"cell_type":"markdown","metadata":{"id":"HIE206heRmGw"},"source":["<h3 align=\"center\">NEU 437/537</h3>\n","<h4 align=\"center\">Princeton University, Spring 2023</h4>\n","\n","---\n","# Homework 3: Generalized Linear Models (GLMs)\n","\n","#### 38 points total (+ 1 bonus point)\n","\n","#### Due: **Friday, April 14 at MIDNIGHT** (*10% off per day late*)\n","---"]},{"cell_type":"markdown","source":["## Formatting Instructions\n","- Please prepare your homework submission completely within your own copy of this colab notebook.\n","\n","- For each problem or sub-problem, please **limit yourself to one Code cell and/or one Markdown cell** as appropriate (switch between them by using the menu at the top, or the shortcuts `Ctrl+M M` for Markdown and `Ctrl+M B` for Code). \n","\n","- **Submitting your homework**:  Please submit an .ipynb file via the assignment tab in Canvas. (From your notebook, File->Download->Download .ipynb).  Late submissions will be penalized 10% per day.\n","\n","- **Test before submmitting**: Before submitting, make sure to verify that your code runs without  errors by selecting `Runtime -> Restart & Run All`. \n","\n","- **Code Hygiene**: Giving variables human-readable names will make your code easier for both you and us to interpret. Similarly, when plotting, give your axes labels (using ```plt.xlabel()``` and ```plt.ylabel()```) and add legends where appropriate (using ```plt.legend()```)."],"metadata":{"id":"qM3wweaLFVck"}},{"cell_type":"markdown","metadata":{"id":"NJOcb99PRmGz"},"source":["## Set up\n","Let's import some of our favorite packages and set up some plotting parameters."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"fh6pnq_tRmG0","executionInfo":{"status":"ok","timestamp":1683474264134,"user_tz":240,"elapsed":15019,"user":{"displayName":"Adrian Bondy","userId":"12236787129817827943"}},"outputId":"a301c2c6-86b3-4967-d71d-42e6808494a5","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyglmnet\n","  Downloading pyglmnet-1.1-py3-none-any.whl (23 kB)\n","Installing collected packages: pyglmnet\n","Successfully installed pyglmnet-1.1\n"]}],"source":["%matplotlib inline\n","\n","import numpy as np\n","import os\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","! pip install pyglmnet # install pyglmnet into colab environment\n","from pyglmnet import GLM\n","\n","\n","# set default font sizes\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 14\n","BIGGER_SIZE = 16\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"]},{"cell_type":"markdown","source":["## Introduction: Fitting Encoding Models to Retinal Ganglion Cells\n","\n","In this problem set, you will fit two **classical GLMs** (linear-Gaussian and Poisson) to spike train data. The dataset contains spike responses from four  parasol **retinal ganglion cells** (RGCs) in the macaque retina stimulated with full-field binary white noise (i.e. the video screen varied randomly between on (high luminance) and off (low luminance) at 120Hz.) \n","\n","\n","<figure>\n","<img src=\"https://github.com/Brody-Lab/neu_437_public/raw/main/PSet3%20%20%20Using%20GLMs%20to%20model%20behavioral%20and%20neural%20data/retina%20diagram.jpg\" class=\"center\" alt=\"schematic illustration of the Poisson Clicks task\" height=500>\n","<figcaption align = \"left\">from: Frisby, J. P. (1980) Seeing, Illusion, Brain and Mind, Oxford University Press ©</figcaption>\n","</figure>\n","\n","RGCs represent the final output stage of the retina. Their spikes travel to the brain via along lengthy axons which are bundled together in the optic nerve. \n","\n","There are many different types of RGCs, varying in their stimulus-response properties and where in the brain they project. One broad division is between ON and OFF RGCs, which are excited by increments or decrements in illumination respectively. In this dataset, cells 1 and 2 are OFF RGCs and cells 3 and 4 are ON RGCs.\n","\n","Our goal is to fit an **encoding model** to the RGCs in this dataset that describes the probability of a neuron spiking given the stimulus.\n","\n","The data consist of three variables:\n","1. `Stim`: a vector describing the stimulus sequence. A value of `1` means the screen was on in that time bin and a value of `-1` means it was off. \n","2. `SpTimes`: `SpTimes[i]` is a vector containing the times (in seconds) when a spike was recorded from the *i*th neuron\n","3. `StimTimes`:  the time (in seconds) corresponding to each time bin \n","\n","----\n","##### ATTRIBUTION:\n","\n","These data were collected by Valerie Uzzell in the lab of\n","E.J. Chichilnisky at the Salk Institute.  For full information see\n","Uzzell et al (J Neurophys 04), or (Pillow et al J Neurosci 2005).\n","Please do not distribute these data beyond the course participants,\n","without permission."],"metadata":{"id":"urm6CypCtR9o"}},{"cell_type":"markdown","source":["## Loading the data\n","\n","Let's run the code cell below to load the dataset."],"metadata":{"id":"H4Up_VwiE_IY"}},{"cell_type":"code","source":["# Data from Uzzell & Chichilnisky 2004\n","# load data from github repo\n","dataset_url = \"https://github.com/Brody-Lab/neu_437_public/raw/main/PSet3%20%20%20Using%20GLMs%20to%20model%20behavioral%20and%20neural%20data/data_RGCs/\"\n","dataset_name = [\"SpTimes.mat\",\"Stim.mat\",\"stimtimes.mat\"]\n","print('Loading %s, %s, and %s from %s\\n' % (dataset_name[0],dataset_name[1],dataset_name[2],dataset_url))\n","for i in range(3):\n","  system_call = \"wget -O {dataset_name} {dataset_url}{dataset_name} > /dev/null\".format(dataset_name=dataset_name[i],dataset_url=dataset_url)\n","  os.system(system_call)\n","Stim = np.squeeze(sio.loadmat('Stim.mat')[\"Stim\"]); \n","StimTimes = np.squeeze(sio.loadmat('stimtimes.mat')[\"stimtimes\"]); \n","SpTimes = np.squeeze(sio.loadmat('SpTimes.mat')[\"SpTimes\"]); \n","Stim[Stim>0] = 1;\n","Stim[Stim<0] = -1;"],"metadata":{"id":"PHGYAt9AM9BX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683474268591,"user_tz":240,"elapsed":2474,"user":{"displayName":"Adrian Bondy","userId":"12236787129817827943"}},"outputId":"5b5094de-be2c-4a73-fa99-eb1aa5a7c29c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading SpTimes.mat, Stim.mat, and stimtimes.mat from https://github.com/Brody-Lab/neu_437_public/raw/main/PSet3%20%20%20Using%20GLMs%20to%20model%20behavioral%20and%20neural%20data/data_RGCs/\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"FGTD7rkQXW1J"},"source":["## Question 1: Computing basic statistics and visualizing the raw data (6 points total)\n","\n","Whenever we set out to analyze a new dataset, it’s a good idea to check the basics and make sure we know what we’re looking at before we move on to more complicated analyses. Otherwise, we might make some assumptions about the structure or properties of the data that simply aren’t true. So let’s see what we’re working with!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z-CSCMBcdII7"},"source":["### Question 1a: basic statistics (2 points)\n","\n","* Using the variable ```StimTimes```, calculate the time interval between consecutive stimuli presentations and save it to the variable `dtStim`. This value is equivalent to the inverse of the display refresh rate.\n","* Calculate the total number of stimulus presentations and save it to the variable `nT`. \n","* Using the variable ```SpTimes```, calculate the number of spikes for cell 3 and save it to the variable `nsp`.\n","\n","<font color=\"red\"> Your answer in code below </font>\n","\n","**(2 points)**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Ok-atocbdDcD","colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"status":"error","timestamp":1683474217912,"user_tz":240,"elapsed":295,"user":{"displayName":"Adrian Bondy","userId":"12236787129817827943"}},"outputId":"8f4744fd-8245-4534-fa7c-2990c873dd42"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-e75d408bb237>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    dtStim = # YOUR CODE HERE # time bin size for stimulus\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["###### YOUR CODE HERE #######\n","\n","# Question 1a\n","\n","# Pick a cell to work with\n","cellnum = 3 # (1-2 are OFF cells; 3-4 are ON cells)\n","# Select the spike times for cell 3\n","tsp = SpTimes[cellnum-1] # spike times for cell 3\n","\n","# Compute some basic statistics on the data\n","dtStim = # YOUR CODE HERE # time bin size for stimulus\n","nT = # YOUR CODE HERE # number of time bins in stimulus\n","nsp = # YOUR CODE HERE # number of spikes\n","\n","# Print out some basic info\n","print('Loaded RGC data: cell %d\\n' %(cellnum))\n","print('Number of stim frames: %d (%.1f minutes)\\n' %(nT,nT*dtStim/60))\n","print('Time bin size: %.1f ms\\n' %(dtStim*1000))\n","print('Number of spikes: %d\\n\\n' %(nsp))"]},{"cell_type":"markdown","metadata":{"id":"hc_KZbnz6HUN"},"source":["### Question 1b: visualizing the stimulus and spike times (2 points)\n"," Now let’s visualize the data. In a single figure, plot:\n","* the first second of the stimulus signal (x-axis: time in seconds, y-axis: stimulus luminance values)\n","* a spike raster plot (i.e. draw a marker at every time point when a spike occurred)\n","\n","<font color=\"red\"> Your answer in code block below</font>\n","\n","**(2 points)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gep2LwhGmFEb"},"outputs":[],"source":["###### YOUR CODE HERE #######\n","\n","# Question 1b\n","max_time_s=1\n","idx = np.where(StimTimes<=max_time_s)\n","idx=idx[0] # idx will be a useful later on. it contains the indices of all time bins within the first second\n","\n","plt.figure(figsize=(13,4))\n","# plot first second of stimulus signal\n","plt.plot(...)\n","# plot when neuron spiked\n","plt.plot(...)\n","\n","plt.xlim(0,1)\n","plt.xlabel('Time (s)')\n","plt.yticks([]);\n","plt.legend([\"Display Luminance\",\"Spike Times\"]);"]},{"cell_type":"markdown","metadata":{"id":"GAQI51iImFEb"},"source":["### Question 1c: from spike times to binned spike counts (2 points)\n","Instead of looking at the raw spike times, let's bin the spikes so we have one spike count corresponding to each time bin of the stimulus. This will make it more straightforward to relate the stimulus (one scalar value per time bin) to the spikes (one count per equivalent time bin).\n","\n","Use the `np.histogram` function to bin the spike times and store the output in a variable `y`. To do this, you'll call ` y,_ = np.histogram(tsp, bins=bin_edges)`, where `tsp` are the spike times for our selected neuron and `bin_edges` are a sequence of increasing times that define the bins. You will have to construct the varaible `bin_edges`. See documentation for `np.histogram` [here](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html)\n","\n","In constructing, `bin_edges`, the center of the first bin should be dtStim/2 and each bin should be `dtStim` seconds wide. \n","\n","Then you'll use the function `spike_count_stem_plot` to make a stem plot of the first second of binned spike counts.\n","\n","<font color=\"red\"> Your answer in code block below </font>\n","\n","**(2 points)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WP5GgiwXmFEb"},"outputs":[],"source":["###### YOUR CODE HERE #######\n","\n","# Question 1c\n","\n","# Generate a vector of spike counts\n","bin_edges = ...\n","y,_=np.histogram(tsp,bins=bin_edges)\n","\n","def spike_count_stem_plot(y):\n","  plt.figure(figsize=(8,4)) \n","  plt.stem(StimTimes[idx],y[idx],'k-',basefmt=' ',label='data')\n","  plt.ylabel('Spike Count per time bin')\n","  plt.xlabel('time (s)');\n","\n","spike_count_stem_plot(y);"]},{"cell_type":"markdown","metadata":{"id":"rsfiI66woqOt"},"source":["## Question 2:  Building the design matrix (5 points total)\n","\n","Retinal ganglion cells respond to changes in luminance. Therefore, a neuron's spike count at time *t* ($y_t$) is not only related to the stimulus at time *t* but to the recent stimulus history before time *t*. This mean that, for each time bin $t$ we want to consider a stimulus vector $\\vec{x_t}$ of recent stimulus history that we think is related to $y_t$. \n","\n","To make our lives easier down the road, we'll form a so-called \"design matrix\" $X$ whose rows are these vectors."]},{"cell_type":"markdown","metadata":{"id":"fuW-vf0YSvMo"},"source":["### Question 2a: building the design matrix (3 points)\n","\n","Using the variable `Stim` that contains the stimulus sequence, make the design matrix $X$ as described above. To do this, you'll need to decide how long each $\\vec{x_t}$ should be (that is, how many bins backwards in time to consider in predicting spike counts). Call this length $ntfilt$ and start with `ntfilt=25`\n","\n","Your design matrix $X$ should therefore be of size $nt$ x $ntfilt$, with row *t* containing the stimulus values from $t-ntfilt+1$ to $t$\n","\n","In doing this, you may find it helpful to first create a “padded” version of `Stim` (call it `padded_Stim`) to which $ntfilt-1$ zeros are appended at the beginning.\n"," \n","Create a plot that shows the first 50 rows of the design matrix $X$. \n","\n","<font color=\"red\"> Your answer in code below </font>\n","\n","**(2 points)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qIYDSVpk6YS"},"outputs":[],"source":["###### YOUR CODE HERE #######\n","\n","# Question 2a\n","\n","# set the number of time bins of stimulus to use for predicting spikes\n","ntfilt = 25 \n","padded_Stim = \n","X = \n","\n","# Let's visualize a small part of the design matrix just to see it\n","plt.figure()\n","plt.imshow(...)\n","plt.title('Design Matrix')"]},{"cell_type":"markdown","source":["\n","What kind of pattern do you see in the design matrix? Why do you see this pattern? \n","\n","<font color=\"red\"> Your answer in red text here </font>\n","\n","**(1 point)**"],"metadata":{"id":"mpPPswFWWJZu"}},{"cell_type":"markdown","metadata":{"id":"aIOEIHrhSwLK"},"source":["### Question 2b: varying the kernel size (2 points)\n","Try different values of $ntfilt$ above and replot the resulting design matrix.\n","\n","How do you expect a model predicting retinal ganglion cell spiking will change based on $ntfilt$ (i.e. the length of stimulus history that we consider)? \n","\n","<font color=\"red\"> Your answer in red here </font>\n","\n","**(2 points)**"]},{"cell_type":"markdown","metadata":{"id":"YQLyXRZuYy3d"},"source":["## Question 3: Linear-Gaussian Encoding Model (15 points total)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J2VWXHfpSxYQ"},"source":["Our overall goal is to fit an encoding model that relates our stimulus history vectors (the set of $\\vec{x_t}$, i.e. the rows of our design matrix $X$) to our spike counts $\\vec{y}$. We'll start as simple as we can, with a linear-Gaussian model:\n","\n","$$ \\vec{\\mu} = X\\vec{k} $$\n","\n","$$ \\vec{y} \\sim \\mathcal{N}(\\vec{\\mu},\\sigma^2I) $$\n","\n","Under this model, the spikes are assumed to be conditionally Gaussian distributed with the mean rate at each time point given by the projection of the vector of recent stimulus history onto a \"kernel\" $\\vec{k}$ (also called a \"filter\" or \"weight vector\"). This kernel describes how spiking depends on each prior time point in our stimulus, up to $ntfilt$ time points back. The kernel is what we are trying to estimate.\n","\n","The maximum likelihood solution for $\\vec{k}$ has a closed form solution under this model (also known as the least-squares regression solution):\n","\n","$$  \\hat{k}_{ML} = (X^TX)^{-1} X^T\\vec{y}  $$\n","\n","The first term in this equation ($X^TX$) is equivalent to the stimulus covariance (call this C). C is a matrix of size $ntfilt$ x $ntfilt$. It quantifies how similar stimulus luminance is between successive frames. It is a symmetric matrix whose diagonal is the variance of X, and whose off-diagonal elements capture the covariance of the stimulus as a function of different time lags. Since our stimulus is white noise (and therefore temporally uncorrelated), we'd expect this matrix to be a scaled version of the identity matrix (ones on the diagonal, zero elsewhere).\n","\n","The second term ($X^T\\vec{y}$) is proportional to the so-called **spike-triggered average (or STA)**. The STA is the mean of the stimulus history vectors $\\vec{x_t}$ corresponding to time points in which the neuron spiked. The STA is an unbiased estimator of $\\vec{k}$ when the stimulus is temporally uncorrelated."]},{"cell_type":"markdown","source":["### Question 3a: compute and visualize the stimulus covariance (2 points)\n","Compute the stimulus covariance $X^TX$ using the variable `X` that you've already computed and plot it as an image. \n","\n","<font color=\"red\"> Your answer in code </font>\n","\n","**(1 point)**"],"metadata":{"id":"DU7naJXOb0qN"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 3a\n","C = \n","plt.figure()\n","plt.imshow(C)\n","plt.colorbar()\n","plt.title('Stimulus Covariance')"],"metadata":{"id":"zCf-OXJCb_Am"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","What does the stimulus covariance look like? Why does it look like this?\n","\n","<font color=\"red\"> Your answer in red text here</font>\n","\n","**(1 point)**"],"metadata":{"id":"2r2UrCbtWDvn"}},{"cell_type":"markdown","source":["### Question 3b: compute and visualize the STA and estimated kernel (2 points)\n","Compute the STA for cell 3 using the variables `X` and `y` you've already calculated. This can be calculated as: $$ STA = \\frac { 1} { n_{sp}} X^T \\vec{y} $$\n","where $n_{sp}$ is the total number of spikes for our selected cell (i.e. variables `nsp`). Then, compute the maximum likelihood estimate for $\\vec{k}$.  Use the provided function `plot_kernel` to visualize them both in the same set of axes.\n","\n","<font color=\"red\"> Your answer in code </font>\n","\n","**(2 points)**"],"metadata":{"id":"WRN4gpTxb9ri"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xo-iuHSQFD0C"},"outputs":[],"source":["###### YOUR CODE HERE #######\n","\n","# Question 3b\n","\n","# calculate STA\n","sta = \n","\n","# linear-gaussian ML estimate of the kernel\n","kernel_gaussian = \n","\n","# Let's plot them both (rescaled as unit vectors so we can see differences in their shape).\n","ttk = np.linspace(-ntfilt,0,ntfilt,endpoint=True)*dtStim # time bin for STA (in seconds)\n","def plot_kernel(kernel,label):\n","  plt.plot(ttk,ttk*0,'k--')\n","  plt.plot(ttk,kernel/np.linalg.norm(kernel),'o-',linewidth=2,label=label) # rescaled as unit vectors so we can see differences in their shape\n","  plt.legend()\n","  plt.xlabel('time before spike (s)')\n","  plt.ylabel('display luminance')\n","\n","plt.figure()\n","plot_kernel(sta,'STA')\n","plot_kernel(kernel_gaussian,'linear-gaussian kernel')"]},{"cell_type":"markdown","source":["### Question 3c: interpreting the STA and estimated kernel (2 points)\n","What does the STA look like? What does this tell us about how this neuron responds to the stimulus? How does the STA compare to the maximum likelihood estimate of $\\vec{k}$ under our linear-Gaussian model? Does this make sense? Why or why not?\n","\n","<font color=\"red\"> Your answer in red text here </font>\n","\n","**(2 points)**"],"metadata":{"id":"VQ6UfpJ7dSym"}},{"cell_type":"markdown","metadata":{"id":"ow80xAZqhGHy"},"source":["### Question 3d: comparing the observed spike counts and predicted rates (2 points)\n","\n","Now let’s see how good the model fit is by comparing the observed spike counts $\\vec{y}$ to the mean firing rates predicted by the model (i.e. $ \\vec{\\mu} = X\\vec{k} $). To do this, replot the binned spike counts from 1c (by calling `spike_count_stem_plot(y)`) and then add a plot, in the same figure, of the predicted firing rates, for the same time period (the 1st second). You may find the variable `idx` defined earlier useful, as this indexes the time bins within this time period.\n","\n","<font color=\"red\"> Your answer in code </font>\n","\n","**(1 point)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ittwum0FD0D"},"outputs":[],"source":["###### YOUR CODE HERE #######\n","\n","# Question 3d\n","\n","# calculate the model predicted firing rates\n","pred_rate_gaussian = \n","\n","# Plot observed versus predicted spike counts\n","spike_count_stem_plot(y)\n","plt.plot(StimTimes[idx],pred_rate_gaussian[idx],linewidth=2, label='linear-gaussian model')\n","plt.legend();"]},{"cell_type":"markdown","source":["\n","What do you see? Describe the relationship between spike count and GLM prediction. Does the model go wrong? If so, where?\n","\n","<font color=\"red\"> Your answer in red text </font>\n","\n","**(1 point)**"],"metadata":{"id":"3-TPlB1IV9zl"}},{"cell_type":"markdown","metadata":{"id":"SA5M1MBLZx18"},"source":["### Question 3e: adding a bias term (3 points)\n","Since our stimulus has zero-mean and we are using a linear model, on average, our predicted spike counts also have zero mean. This is obviously not what we want since the neurons have non-zero mean spike count. Including a \"bias\" (sometimes called \"offset\") term will fix this. The linear-Gaussian model with a bias term has a mean rate given by:\n","\n","$$ \\vec{\\mu} = X\\vec{k} + β_0 $$\n","where $β_0$ is the bias term (an additional parameter to estimate). In practice, the way this is implemented is by prepending a column of ones to our design matrix $X$. Then we fit the model exactly as before (without adding $β_0$). This time the kernel $\\vec{k}$ that we estimate will have one additional element at the beginning, which works just like a bias term.\n","\n","Create a new design matrix (call it `X_offset`) with a column of 1’s added at the beginning. Then calculate a new estimate of the kernel (call it `kernel_gaussian_offset`, whose first element is the bias estimate) and a new prediction for the spike counts (call this `pred_mean_gaussian_offset`) under this new model.\n","\n","Copy the last bit of code from Question 3d that plots the observed spike counts and predicted firing rates over the first second of the stimulus. Then add a new line to additionally plot the predicted rates under the new model that includes an offset term.\n","\n","<font color=\"red\"> Your answer in code below</font>\n","\n","**(2 points)**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qr_tTUVRCY96"},"outputs":[],"source":["###### YOUR CODE HERE #######\n","\n","# Question 3e\n","\n","# design matrix with offset\n","X_offset = ...\n","\n","# linear-gaussian ML estimate of the kernel\n","kernel_gaussian_offset = ...\n","\n","# calculate the model predicted firing rates\n","pred_rate_gaussian_offset = ...\n","\n","# Plot predicted rates and observed counts\n","# YOUR CODE HERE\n"]},{"cell_type":"markdown","source":["What do you see? Does adding an offset improve the model?\n","\n","<font color=\"red\"> Your answer in red text here</font>\n","\n","**(1 point)**"],"metadata":{"id":"dRigiLdfV5bG"}},{"cell_type":"markdown","metadata":{"id":"AHyIcvlDtFTF"},"source":["### Question 3f: Quantifying model accuracy (2 points)\n","\n","The mean squared error is one useful metric of model accuracy: $$MSE = \\frac{1}{nT} \\sum_t(y_t - \\mu_t)^2  $$\n","where $nT$ is the total number of time bins. In general, a better model will have smaller MSE. Calculate MSE for the linear-Gaussian models we've fit, both with and without an offset. In the formula above, $\\mu_t$ is just the *t*th time bin of the predicted firing rates given by `pred_rate_gaussian` or `pred_rate_gaussian_offset`.\n","\n","<font color=\"red\"> Your answer in code </font>\n","\n","**(1 point)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXkQtfTUt0Kq"},"outputs":[],"source":["###### YOUR CODE HERE #######\n","\n","# Question 3f\n","\n","mse_gaussian = # model with no bias\n","mse_gaussian_offset = # model with bias\n","\n","print('Mean squared error: lin-gauss GLM. no offset: %.2f\\n' %mse_gaussian)\n","print('Mean squared error: lin-gauss GLM. with offset: %.2f\\n' %mse_gaussian_offset)"]},{"cell_type":"markdown","source":["What do these results tell us? \n","\n","<font color=\"red\"> Your answer in red text here</font>\n","\n","**(1 point)**"],"metadata":{"id":"rhJU2pByV0c3"}},{"cell_type":"markdown","metadata":{"id":"8qN1ruufzYZC"},"source":["### Question 3g: Varying the kernel size, revisited (2 points)\n","\n","Now go back to Question 2b and try several different values of $ntfilt$. Remember, $ntfilt$ controls how far back in time we are considering the stimulus history in our model (i.e. how temporally extended our kernel is). For each value of $ntfilt$ rerun all the code cells up to this point and see how things change.\n","\n","Describe visually how the estimated kernel changes with $ntfilt$. How is MSE affected? What (approximately) seems be an optimal value of $ntfilt$, such that increasing $ntfilt$ further adds no additional benefit? How does your answer compare to what you predicted in Question 2b?\n","\n","<font color=\"red\"> Your answer in text here </font>\n","\n","**(2 points)**\n"]},{"cell_type":"markdown","metadata":{"id":"30hFKKG52afp"},"source":["## Question 4: Poisson encoding model (8 points total + 1 bonus point)\n","\n","Next we'll use a Poisson GLM as our encoding model:\n","\n","$$ \\vec{\\lambda} = f (X \\vec{k} + β_0) $$\n","\n","$$ \\vec{y} \\sim Poiss( \\vec{\\lambda} ) $$\n","\n","Under this model, the spike counts are assumed to be conditionally Poisson distributed with a mean rate that is a nonlinear function of the linear projection we used in the linear-Gaussian model.  $f$ is a pointwise nonlinearity that transforms the scalar output of the linear stage of the model into a positive number suitable as the mean of a Poisson distribution. The canonical choice for $f$ in a Poisson GLM is the exponential function.\n","\n","The maximum likelihood solution for $\\vec{k}$ does not have a closed form solution under this model, but the likelihood is guaranteed to be convex, so it can be obtained using gradient ascent. (And, as we'll see next, there are Python packages that make fitting a poisson GLM trivial)."]},{"cell_type":"markdown","metadata":{"id":"BpR5K5J3PSy1"},"source":["### Question 4a: fitting a Poisson GLM (4 points)\n","\n","Fit a Poisson encoding model using the same values for `X` and `y` as before. You will use the `GLM` class from the `pyglmnet` package. You don't need to manually add a column of ones. $β_0$ is estimated automatically. \n","\n","**Documentation for the `GLM` class is [here](http://glm-tools.github.io/pyglmnet/api.html). You will need to read it to be able to do Question 4.**\n","\n","Create a variable called `kernel_poisson` which contains the estimate for $\\vec{k}$. Then create a variable called `beta_0_poisson` which contains the estimated offset term. \n","\n","Make a plot of the estimated kernel for the \"linear-Gaussian with offset\" model and the poisson model, in the same figure. You can again use the function `plot_kernel` defined earlier. (For the \"linear-Gaussian with offset\" case, don’t include the offset weight in your plot, i.e. use `kernel_gaussian_offset[1:]`.\n","\n","<font color=\"red\"> Your answer in code </font>\n","\n","**(3 points)**"]},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 4a\n","\n","# use the GLM class from pyglmnet to fit a Poisson GLM\n","glm_poisson = GLM(distr='poisson',\n","                  learning_rate=1,\n","                  reg_lambda=0)\n","\n","# fit GLM\n","# YOUR CODE HERE\n","# see docs for GLM class at: http://glm-tools.github.io/pyglmnet/api.html\n","\n","# get the estimated kernel (i.e. the fitted model weights)\n","kernel_poisson = ...\n","\n","# get the estimated offset\n","beta_0_poisson = ...\n","\n","# make plots comparing kernels estimated using the linear-Gaussian and Poisson model \n","# use the plot_kernel() function defined earlier\n","plt.figure()\n","plot_kernel(...)"],"metadata":{"id":"8lA-DuvGhKJc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How do the two kernel estimates compare? Is this what you expected?\n","\n","<font color=\"red\"> Your answer in red text </font>\n","\n","**(1 point)**"],"metadata":{"id":"it2eYdxqVu7R"}},{"cell_type":"markdown","source":["### Question 4b - comparing observed spike counts and Poisson-predicted rates (2 points)\n","Obtain the predicted firing rates (i.e. $\\vec{\\lambda}$) under the Poisson model. (N.B. You can easily obtain predictions using the `GLM` class without calculating them yourself). Save the output to a variable called `pred_rate_poisson`. Then copy the last bit of code from Question 3e that plots the observed spike counts and predicted rates over the first second of the stimulus. Add a new line to additionally plot the predictions under the Poisson model.\n","\n","<font color=\"red\"> Your answer in code below </font>\n","\n","**(2 points)**"],"metadata":{"id":"qXFDBhSKSV64"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 4b\n","\n","# predict firing rates from Poisson model\n","pred_rate_poisson = ...\n","\n","# plot predictions compared to the observed spike counts and the predictions fom the linear-gaussian model w/ offset\n","# YOUR CODE HERE (copy from your answer to 3e and then add a line for the Poisson predictions)"],"metadata":{"id":"W4Cq7RZIPOdq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 4c - comparing performance of the linear-Gaussian and Poisson models (2 points + 1 bonus point)\n","\n","Calculate the MSE of the poisson GLM. \n","\n","<font color=\"red\"> Your answer in code below </font>\n","\n","**(1 point)**\n"],"metadata":{"id":"G1dbMYpZOVHX"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 4C\n","\n","mse_poisson = ... # mean squared error, poisson GLM\n","print('Training perf (MSE): exp-poisson GLM: %.2f\\n' %mse_poisson)"],"metadata":{"id":"u3wTJIoKVc8C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Based on the MSE, and on the plot you made in 4b, which type of GLM is a better fit to the data? Why do you think this is true?\n","\n","<font color=\"red\"> Your answer in red text </font>\n","\n","**(1 point)**\n","\n","OPTIONAL BONUS QUESTION: While easy to calculate and intuitive to understand, mean-squared error is generally not considered the fairest way of comparing the performance of different types of GLMs. This is because it is biased towards favoring a particular noise model. Which one and why?\n","\n","<font color=\"red\"> Your answer in red text </font>\n","\n","**(1 bonus point)**"],"metadata":{"id":"mQuNRsnvVRH_"}},{"cell_type":"markdown","source":["## Question 5: The nonlinearity: room for improvement? (9 points total)\n","\n","So far, we've used an exponential nonlinearity in our Poisson encoding model by default. Is this optimal? In this question, we'll address this question empirically."],"metadata":{"id":"JXoznbZCjWRv"}},{"cell_type":"markdown","source":["### Question 5a: empirically estimating the nonlinearity (4 points)\n","We'll approach Question 5 by first directly comparing the output of the linear stage of our model (i.e. $X\\vec{k}+β_0$, the \"linear predictor\") to the observed spike counts ($\\vec{y}$). If an exponential nonlinearity is appropriate this relationship should be exponential in shape.\n","\n","Break trials down into 15 groups based on the value of the output of the linear stage of the model (the \"linear predictor\"). To do this, first calculate the vector of linear predictors using the estimate of $\\vec{k}$ obtained from our Poisson model (i.e. the variable `kernel_poisson`) and the offset term (i.e. the variable `beta_0_poisson`). Save the vector of linear predictors to a variable `linear_predictor`.\n","\n","Break these values into 15 bins. `np.histogram` will give you the bin edges, and `np.digitize` will tell you which bin each trial is in. Next, find the center of each bin and save it to a variable called `fx`. Finally, looping over the 15 groups, find the average spike count associated with each group. Assign these mean spike counts to a variable `fy`.\n","\n","Plot `fy` against `fx` (i.e. `fx` on the x-axis and `fy` on the y-axis). This will give us a picture of how spike counts depend on the value of the linear predictor.  In the same plot, plot `np.exp(fx)` to compare against the nonlinear mapping we actually used.\n","\n","<font color=\"red\"> Your answer in code below </font>\n","\n","**(3 points)**"],"metadata":{"id":"TedE9uuAXbR1"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 5a\n","\n","# number of bins for parametrizing the nonlinearity f\n","nfbins = 15\n","\n","# compute linear predictor\n","linear_predictor = ...\n","\n","# bin linear predictors\n","\n","# assign bin centers to fx\n","fx = ...\n","\n","# now compute mean spike count in each bin, assign to fy\n","fy = np.zeros((nfbins)) \n","for i in range(nfbins):\n","    fy[i] = np.mean( ... )\n","\n","# plot fy and exp(fx) against fx and label axes\n","plt.plot(...)\n","plt.xlabel('linear predictor')\n","plt.ylabel('spike count/bin')\n","plt.legend(('$e^x$', 'empirical f'), loc=0)\n"],"metadata":{"id":"UYOve4Qj3FvJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How well does the exponential nonlinearity actually capture the relationship between the output of the linear stage of the model and spike count? In the plot you just made, try using log-scaling on the y-axis with `plt.yscale(\"log\")`. Does this affect your thinking?\n","\n","<font color=\"red\"> Your answer in red text here </font>\n","\n","**(1 point)**"],"metadata":{"id":"Kk69JD0tVcN8"}},{"cell_type":"markdown","source":["### Question 5b: comparing nonlinearities using log likelihoods (3 points)\n","\n","The observed relationship between the linear predictor and oberved spike counts that we plotted above is currently only defined for the 15 bins we specified. Below, we define a function `fnlin` which uses linear interpolation to assign a predicted rate for any value of the linear predictor. This will give us a nonparametric estimate of the nonlinearity that we can actually use to predict firing rates on single trials. "],"metadata":{"id":"xtbnfS1O3F4O"}},{"cell_type":"code","source":["# Define a function we can evaluate at any point that interpolates between the sample points defined by fx and fy\n","def fnlin(x):\n","    return np.interp(x,fx,fy)\n","\n","# use this non-parametric estimate of the nonlinearity to get predicted firing rates\n","pred_rate_poisson_np = fnlin(linear_predictor);"],"metadata":{"id":"2ynLpfjUl9EO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's compare how the predicted rates using the non-parametric estimate of the nonlinearity (`pred_rate_poisson_np`) compare to the predictions generated using the exponential nonlinearity (`pred_rate_poisson`), in terms of how well they match the data.\n","\n","This time, instead of using MSE as our metric of model accuracy, we'll be slightly more sophisticated and compare model log-likelihoods.\n","\n","The likelihood of our fitted Poisson encoding model is just the conditional probability of observing the spike counts $\\vec{y}$ given model-predicted rates $\\vec{\\lambda}$:\n","\n","$$ \\begin{aligned} \n","\\mathcal{L}(\\vec{\\lambda}|\\vec{y})= P(\\vec{y}|\\vec{\\lambda})&= \\prod_tPoiss(y_t;\\lambda_t)\\\\&=\\prod_t\\frac{{\\lambda_t}^{y_t}e^{-\\lambda_t}}{y_t!}\n","\\end{aligned}\n","$$\n","\n","Therefore, the log-likelihood is given by:\n","$$  \\begin{aligned} \n","\\mathcal{l}(\\vec{\\lambda}|\\vec{y})&=ln\\prod_t\\frac{{\\lambda_t}^{y_t}e^{-\\lambda_t}}{y_t!}\\\\&=\\sum_ty_t~ln\\lambda_t-\\lambda_t+c\n","\\end{aligned}\n"," $$\n","where $c$ is a term that does not depend on the model parameters, and so can be ignored for our purposes. (N.B. The above derivation of the log-likelihood makes the dubious assumption of conditional independence of neural spiking across time bins).\n","\n","Ignoring $c$, we can write the log-likelihood in matrix-vector notation as:\n","\n","$$ \\mathcal{l}(\\vec{\\lambda}|\\vec{y})=\\vec{y} \\cdot ln\\vec{\\lambda} - \\sum\\vec{\\lambda} $$\n","\n","Using this formula, calculate the log-likelihood for the two Poisson encoding models we want to compare. To do this, substitute `pred_rate_poisson` or `pred_rate_poisson_np` for $\\vec{\\lambda}$.\n","\n","<font color=\"red\"> Your answer in code </font>\n","\n"," **(2 points)**\n"],"metadata":{"id":"fsiMdxM6l9xO"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 5b\n","\n","ll_poisson_exp = ...\n","ll_poisson_np = ...\n","\n","print('Poisson log-likelihood with exponential nonlinearity: %f' %(ll_poisson_exp))\n","print('Poisson log-likelihood with nonparametric nonlinearity: %f' %(ll_poisson_np))"],"metadata":{"id":"5fOORLwIIQWz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Log-likelihoods are negative. The greater -- i.e. the less negative -- the better. Based on the log-likelihoods, which nonlinearity better fits the data? Are you surprised by this? Why or why not?\n","\n","<font color=\"red\"> Your answer in red text </font>\n","\n","**(1 point)**"],"metadata":{"id":"YvaYBOWbVjZx"}},{"cell_type":"markdown","source":["### Question 5c: big picture thoughts (2 points)\n","Thinking big picture, what do your findings from Questions 4 and 5 tell you about using GLMs for modeling neural data? What aspects of model selection are important to consider?\n","\n","<font color=\"red\"> Your answer in red text </font>\n","\n","**(2 points)**"],"metadata":{"id":"nz-TaGAe38Tx"}}],"metadata":{"colab":{"provenance":[{"file_id":"1MRD51btOhTJ-H9dgCOkGLYbuT3N_vxLl","timestamp":1680143418298},{"file_id":"1NhPjmGBhIVpHwnlv-Fq9cPegbt79B3t1","timestamp":1679513964928}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}