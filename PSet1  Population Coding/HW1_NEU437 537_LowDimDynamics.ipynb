{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oumWTnr8i7K-lNdJHLfvoYg1q879XOMf","timestamp":1676479016404},{"file_id":"1RXYu7Cu5P-che5Io9IRgGO1xgIxup-hK","timestamp":1676478827209},{"file_id":"1Qz2pLzO2bsyVvYVPqcEkXtDWgKnwtA0E","timestamp":1645685456506},{"file_id":"1GKo8KzYsB2AZqy57ccysivxhIr8lgl38","timestamp":1638070309635},{"file_id":"1FE0dlKvUb_oF4nlHtvTzi2rYPfDBdwHQ","timestamp":1636475622728}],"collapsed_sections":["72j7_QX1pbpM"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HIE206heRmGw"},"source":["<h3 align=\"center\">NEU 437/537</h3>\n","<h4 align=\"center\">Princeton University, Spring 2023</h4>\n","\n","---\n","# Homework 1: Low Dimensional Dynamics during Decision Making\n","#### Due: **Friday, Mar 3rd at MIDNIGHT** (*10% off per day late*)\n","---\n"]},{"cell_type":"markdown","source":["## Instructions\n","- Go to the menu File->Save a copy in Drive to make your own copy of the notebook that you can run and modify. Please prepare your homework submission completely within your own copy of that colab notebook.\n","\n","- For each problem or sub-problem, please **limit yourself to one Code cell and/or one Markdown cell** as appropriate (switch between them by using the menu at the top, or the shortcuts `Ctrl+M M` for Markdown and `Ctrl+M B` for Code). \n","\n","- **Submitting your homework**:  Please submit an .ipynb file via the assignment tab in Canvas. (From your notebook, File->Download->Download .ipynb).  Late submissions will be penalized 10% per day.\n","\n","- **Test before submmitting**: Before submitting, make sure to verify that your code runs without  errors by selecting `Runtime -> Restart & Run All`. \n","\n","- **Code Hygiene**: Giving variables human-readable names will make your code easier for both you and us to interpret. Similarly, when plotting, give your axes labels (using ```plt.xlabel()``` and ```plt.ylabel()```).\n","\n","- **Looking up Documentation**: In several places, you are given suggestions for pre-existing Python packages to make use of to complete the assignment. Links are provided to documentation in those cases, which you will need to click on and read through."],"metadata":{"id":"xhfA6n7iE-F9"}},{"cell_type":"markdown","metadata":{"id":"NJOcb99PRmGz"},"source":["## Setup\n","\n","Import necessary packages and set plotting constants"]},{"cell_type":"code","metadata":{"id":"fh6pnq_tRmG0"},"source":["# interactive debugger. uncomment this if you want errors to stop your code and give you interactive access to the workspace\n","%pdb off \n","\n","# import packages\n","import os\n","import numpy as np\n","import scipy.io as sio\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from statsmodels.stats.proportion import proportion_confint\n","\n","# set default font sizes\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 14\n","BIGGER_SIZE = 16\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Introduction: Neural Population Recordings during Auditory Evidence Accumulation in Rats\n","\n","In this problem set we will be exploring dimensionality reduction of neural population recordings. \n","\n","We'll be analyzing a real dataset (documented further [here](https://github.com/Brody-Lab/adrian_striatum_analysis)), that consists of neural recordings from a rat performing an auditory decision making task called the [\"Poisson Clicks\" task](https://www.science.org/doi/10.1126/science.1233912?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed/) (see Fig. 1). \n","\n","**IMPORTANT NOTE: We ask that you do not distribute or make use of these data outside the context of this problem set.**\n","\n","<figure>\n","<img src=\"https://github.com/Brody-Lab/adrian_striatum_analysis/raw/master/images/Poisson%20Clicks%20Task.jpeg\" class=\"center\" alt=\"schematic illustration of the Poisson Clicks task\" height=330>\n","<figcaption align = \"left\"><b>Figure 1.</b> Schematic illustration of the Poisson Clicks Task</figcaption>\n","</figure>\n","\n","<figure>\n","<img src=\"https://iiif.elifesciences.org/lax/59716%2Felife-59716-fig1-v2.tif/full/1500,/0/default.jpg\" class=\"center\" alt=\"schematic illustration of the Poisson Clicks task\" height=330 >\n","<figcaption align = \"left\"><b>Figure 2.</b> Chronic Neuropixels Recordings in Rats. Figure taken from <a href=\"https://elifesciences.org/articles/59716/\">Luo*, Bondy* et al (2020). eLife</a>. </figcaption>\n","</figure>\n","\n","\n","Briefly here's how the task works. The rat was placed in a behavior box with three \"ports\" it could interact with by inserting its nose into them. To initiate a trial, the rat had to poke into the center port and hold its nose there. After a variable delay, the rat was presented with a sequence of randomly timed auditory clicks from speakers to its left and right. The sequence of clicks could last from 0.2s to 1s.  At the end of the stimulus, the rat had to report whether there were more clicks to the left or right by poking its nose in either the left or right side port. The rat was motivated to perform the task through delivery of liquid reward for a correct choice. The task is designed to force the rats to gradually integrate auditory \"evidence\" provided by the experimenters in the form of the clicks. Thus it allows us to control the decision process while we record from the animal's brain.\n","\n","The rat had a [Neuropixels](https://www.nature.com/articles/nature24636) probe chronically implanted in a part of its brain called the anterior dorsal striatum (ADS). Neuropixels probes are silicon probes with around 1000 recording sites distributed along a 1cm shank. We were able to record simultaneously from many ADS neurons at once (~130 in this example dataset) using this approach (see Fig. 2 for a visual depiction of the recording method). We chose ADS because it [had been previously shown](https://elifesciences.org/articles/34929) to contain single neurons whose firing rates correlate with the value of accumulated evidence for the animal's decision.\n","\n","In this problem set, you'll go beyond single neuron responses to explore the representation of the evolving decision at the population level.\n"],"metadata":{"id":"72j7_QX1pbpM"}},{"cell_type":"markdown","source":["## Loading the data\n","\n","First we'll load the data from Github. "],"metadata":{"id":"Tb19ZTIot2U7"}},{"cell_type":"code","metadata":{"id":"0TPij9nDmFEa","pycharm":{"name":"#%%\n"}},"source":["# load data from github repo\n","dataset_url = \"https://github.com/Brody-Lab/adrian_striatum_analysis/raw/master/datasets/\"\n","dataset_name = \"pset1_data.mat\"\n","print('Loading %s from %s\\n' % (dataset_name,dataset_url))\n","system_call = \"wget -O {dataset_name} {dataset_url}{dataset_name} > /dev/null\".format(dataset_name=dataset_name,dataset_url=dataset_url)\n","os.system(system_call)\n","pset1_data = sio.loadmat(dataset_name,mat_dtype=True)\n","\n","# load variables into workspace, removing useless dimensions and performing type conversions when necessary\n","def load_vars(data,var_names):\n","  vals = [];\n","  for i in range(len(var_names)):\n","    vals.append(np.squeeze(data[var_names[i]]))\n","  return tuple(vals) \n","\n","time_s,rat,sess_date,resolution_s,n_left_clicks,n_right_clicks,stim_off,went_right,response,smooth_std_s,is_correct = \\\n","  load_vars(pset1_data,['time_s','rat','sess_date','resolution_s','n_left_clicks','n_right_clicks','stim_off','went_right','spikes','smooth_std_s','is_correct'])\n","stim_off = stim_off.astype('int');\n","went_right = went_right==1\n","evidence_strength =np.log(n_right_clicks/n_left_clicks) ;\n","n_evidence_bins = 6;\n","evidence_strength_bins = np.percentile(evidence_strength,np.linspace(0,100,n_evidence_bins+1))  #define 7 bins for breaking up trials according to momentary evidence strength\n","time_range_s=[0.1,0.75] # min and max time after stimulus onset (in seconds) to plot and for certain analyses\n","\n","# get and print some basic information about the dataset\n","ntrials,nbins,ncells = response.shape\n","print('Dataset for rat %s from %s includes:\\n - %d trials\\n - %.1f s of spike data per trial\\n - %d cells\\n\\nTemporal resolution is %.0f ms per time bin.\\n\\nSmoothed with %d ms causal Gaussian filter.' \\\n","      % (rat,sess_date,ntrials,nbins*resolution_s,ncells,resolution_s*1e3,smooth_std_s*1e3) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","<h3>The data consists of several key variables, described in detail below.\n","\n","*(TLDR: Feel free to skim this for now and come back to it as a reference as you work through the problem set.)*\n","\n","---\n","\n","**```response```**: an array of size ```ntrials``` $\\times$ ```nbins``` $\\times$ ```ncells```, where ```ntrials``` is the number of trials, ```nbins``` is the number of time bins per trial and ```ncells``` is the number of recorded cells. These are the recorded spiking data in response to the clicks stimulus, broken down by trial, timepoints and cell number. The raw spike counts have been smoothed with a causal half-Gaussian filter with 75 ms s.d. and divided by the bin duration (see ```resolution_s``` below) so that the values represent a smoothed estimate of instantaneous firing rate in spikes per second. The data is aligned to the time of stimulus onset (i.e. the first click) and extends for 1s. Each bin represents 5ms. Stimulus duration was variable, ranging from 0.2 s to 1s. On trials with stimulus duration shorter than 1s, the times after stimulus offset are filled with NaNs to maintain a uniform array size.\n","\n","**```n_left_clicks```**: a vector of integers of length ```ntrials``` giving the number of left clicks on each trial.\n","\n","**```n_right_clicks```**: a vector of integers of length ```ntrials``` giving the number of right clicks on each trial.\n","\n","**```evidence_strength```**: a numeric vector of length ```ntrials``` giving the instantaneous evidence strength for each trial. This is analogous to motion coherence in the classic \"random dot motion\" task, and is equal to $\\log(\\frac{n\\_right\\_clicks}{n\\_left\\_clicks})$.\n","\n","**```time_s```**: a numeric vector of length ```nbins``` giving the time (in seconds relative to stimulus onset) of each time bin \n","\n","**```time_range_s```**: a numeric vector of length 2 giving the starting and ending time (in seconds relative to stimulus onset) of data for plotting purposes and certain analyses. The default value is [0.1,0.75]. Times earlier than this contain little information about the decision and times after this contain too few trials for sufficient statistical power.\n","\n","**```resolution_s```**: a numeric scalar giving the bin duration in seconds (0.005 in this case)\n","\n","**```went_right```**: a Boolean vector of length ```ntrials``` specifying the animal's choice (```True``` for right, ```False``` for left).\n","\n","**```is_correct```**: a Boolean vector of length ```ntrials``` specifying whether the animal's choice was rewarded (i.e. if the animal chose the side with more clicks)."],"metadata":{"id":"mA6DetD-uKBi"}},{"cell_type":"markdown","source":["## Examining the dataset\n"],"metadata":{"id":"ZWrdFtsMMtPw"}},{"cell_type":"markdown","source":["### Plotting the psychometric curve\n","\n","First, let's verify that the animal can do the task. We'll make a plot showing how the subject's choices depended on the amount of accumulated evidence in the stimulus. We expect this to show that the rat reliably choose the side with more evidence, and did so more reliably when the accumulated evidence was stronger. This kind of plot is called a \"psychometric curve\" and we'll define an aptly named function (```plot_psychometric_curve```) to generate one.\n","\n"],"metadata":{"id":"x2u_6PUktchz"}},{"cell_type":"code","source":["def plot_psychometric_curve(signal,went_right,bins=9):\n","\n","  # calculate binned choice fractions\n","  edge = np.max(np.abs(signal))+1\n","  bin_edges = np.linspace(-edge,edge,bins+1)\n","  choice_frac = np.zeros(bins)\n","  n = np.zeros(bins)\n","  for i in range(0,bins):\n","    idx = (signal>bin_edges[i]) & (signal<=bin_edges[i+1])\n","    n[i] = np.count_nonzero(idx)\n","    choice_frac[i] = np.mean(went_right[idx])\n","  bin_mid=(bin_edges[:len(bin_edges)-1]+bin_edges[1:])/2\n","  \n","  # get binomial confidence intervals\n","  err = proportion_confint(count=n*choice_frac, nobs=n, alpha=0.05)  \n","  \n","  # plot\n","  plt.figure()\n","  plt.errorbar(bin_mid,choice_frac,yerr=abs(err-choice_frac),marker=\"o\")\n","  plt.ylim(0,1)\n","  plt.xlim(bin_mid[0]-1,bin_mid[-1]+1)\n","  plt.xlabel(\"# right - # left clicks\")\n","  plt.ylabel(\"Fraction chose right\")\n","\n","plot_psychometric_curve(n_right_clicks - n_left_clicks,went_right)"],"metadata":{"id":"Qhubx6yq3z-h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Indeed, this shows that the animal's choices are systematically modulated by the total stimulus evidence (i.e. the difference between number of right and left clicks). This is one (but by no means the only) piece of evidence demonstrating that rats do indeed perform gradual evidence accumulation to perform the Poisson Clicks task."],"metadata":{"id":"qDuI2u7vIMed"}},{"cell_type":"markdown","source":["### Looking at PETHs\n","\n","As we said earlier, we know that single neurons in ADS (like those in monkey parietal cortex during the classic random dot motion task) have firing rates that correlate with the value of accumulated evidence. To visualize this, we can plot the firing rate of a neuron as a function of time since the onset of the stimulus. Such plots are called \"peri-event time histograms\" (or PETHs).\n","\n","We'll break down trials into four groups based on the strength of momentary evidence (i.e. the value of ```evidence_strength```, defined below) and only include correct trials. "],"metadata":{"id":"AhMsJuHWtWFI"}},{"cell_type":"code","source":["### THIS CODE CELL DEFINES VARIABLES AND FUNCTIONS NEEDED FOR PLOTTING PETHs. \n","### YOU DON'T NEED TO HAVE A DEEP UNDERSTANDING OF WHAT IS GOING ON HERE TO COMPLETE THE PROBLEM SET. \n","\n","# define some constants needed for PETH plotting\n","example_cells=[3,21,26,108,109] # example choice-selective cells for T219_12_20_2019 pset1_data.mat (zero-based index)\n","legend_str=[\"strong left evidence\",\"middle left evidence\",\"weak left evidence\",\"weak right evidence\",\"middle right evidence\",\"strong right evidence\"] # trial condition names\n","cmap = mpl.cm.get_cmap('viridis')  \n","colors=cmap(np.linspace(0,1,n_evidence_bins))  \n","min_time_bin = np.count_nonzero(time_s<=time_range_s[0])\n","max_time_bin = np.count_nonzero(time_s<time_range_s[1])\n","\n","def get_peth(response,nboot=int(1e2),q=[25,75]):\n","  # calculates the PETH (internal function used by plot_peth_example_cells)\n","  # response should be an array of size ntrials x nbins (i.e. the response of one cell for all trials and timepoints)\n","  # nboot defines the number of bootstrap resamples to use for calculating error bars\n","  # q defines the bootstrap coverage interval of the error bar in percentiles\n","  boots=np.empty((nboot,max_time_bin-min_time_bin))\n","  peth = np.empty((n_evidence_bins,max_time_bin-min_time_bin))\n","  peth[:] = np.nan\n","  lower = peth.copy()\n","  upper = peth.copy()\n","  for i in range(0,n_evidence_bins):\n","    trial_idx = (evidence_strength>=evidence_strength_bins[i]) & (evidence_strength<evidence_strength_bins[i+1]) & is_correct;\n","    this_response = response[trial_idx,min_time_bin:max_time_bin]\n","    ntrials=np.count_nonzero(trial_idx)\n","    for k in range(0,nboot):\n","      idx = np.random.randint(low=0, high=ntrials, size=(ntrials,));\n","      boots[k,:] = np.nanmean(this_response[idx,:],axis=0)\n","    lower[i,:]=np.percentile(boots,q[0],axis=0)\n","    upper[i,:]=np.percentile(boots,q[1],axis=0)\n","    peth[i,:] = np.nanmean(this_response,axis=0)    \n","  return peth, lower, upper\n","\n","def plot_peth(data,xlabel=\"Time after stimulus onset (s)\", ylabel=\"Spikes/s\", legend_str = legend_str):\n","  # makes a figure with a subplot for each cell,\n","  # illustrating its smoothed response to the stimulus,\n","  # with trials broken down by signal strength.\n","  # data is an array of size ntrials x nbins x m where m indexes cells. \n","  # cells indexes which cells in data to be plotted.\n","  xs=np.arange(min_time_bin,max_time_bin)*resolution_s     \n","  peth,lower,upper = get_peth(data)\n","  plt.gca().set_prop_cycle('color',colors)     \n","  for i in range(0,n_evidence_bins):   \n","    plt.fill_between(xs,lower[i,:],upper[i,:])    \n","  plt.xlabel(xlabel)\n","  plt.ylabel(ylabel)  \n","  plt.legend(legend_str);    \n","\n","def plot_peth_example_cells(data,cells):\n","  # makes a figure with a subplot for each cell,\n","  # illustrating its smoothed response to the stimulus,\n","  # with trials broken down by signal strength.\n","  # data is an array of size ntrials x nbins x m where m indexes cells. \n","  # cells indexes which cells in data to be plotted.\n","  fig, axs = plt.subplots(1, len(cells), figsize=(30,6))\n","  for cellno in range(0,len(cells)):\n","    plt.sca(axs[cellno])\n","    plot_peth(np.squeeze(data[:,:,cells[cellno]]))\n","    axs[cellno].set_title((\"cell {cellno:d}\").format(cellno=cells[cellno]));\n","    if cellno>0:\n","      plt.xlabel(\"\")\n","      plt.ylabel(\"\")  \n","      plt.legend([]);           \n","\n","plot_peth_example_cells(response,example_cells)       "],"metadata":{"id":"Up_ECpWp0RK0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","You can see that the example neurons each has a preferred choice -- that is, they fire more on trials when the evidence favored that choice. More importantly, the rate at which their firing rate changes depends on the momentary strength of evidence favoring that choice.\n","\n","From data like these, we conclude that these neurons reflect the value of an internal \"decision variable\" that is integrating the momentary sensory evidence and which could be used by the animal to decide which way to go at the end of the trial."],"metadata":{"id":"Aw3qCSW_H69R"}},{"cell_type":"markdown","source":["## Question 1: Reducing the dimensionality of the data"],"metadata":{"id":"OGQQQ-t5uaV5"}},{"cell_type":"markdown","source":["If it's really the case, as these PETHs seem to indicate, that ADS neuron responses reflect a \"latent\" decision process, we should be able to reduce the dimensionality of the data without losing too much information.\n","\n","To do this, we'll start by performing principal components analysis (PCA). This is an unsupervised approach which finds an orthogonal set of axes in neural state space such that the data has most variance along axis 1, the second most along axis 2, and so forth. These new axes are called the \"principal components.\" By confining subsequent analysis to just the top set of principal components, we get a low-dimensional representation of the data that preserves the maximum variance.\n","\n"],"metadata":{"id":"ydLJo6G7WgSg"}},{"cell_type":"markdown","source":["### Data preprocessing\n","Before we perform PCA, we'll need to do a bit of data wrangling/processing.\n","\n","First, while we want to perform PCA on a response vector $\\vec{r}(t)$ where $t$ can be any time during the recording, the variable ```response``` is an array of size ```ntrials``` $\\times$ ```nbins``` $\\times$ ```ncells```. The computation will be much easier if we define a version of ```response``` has a single time dimension that concatenates across all timepoints in the dataset.  Let's define a variable called ```r_t``` that contains the same data as ```response``` but has dimensions (```ntrials``` $*$ ```nbins```) $\\times$ ```ncells```. \n","\n"],"metadata":{"id":"_dzGzwBMxCE8"}},{"cell_type":"code","source":["r_t = response.reshape((ntrials*nbins,ncells))"],"metadata":{"id":"3std1_Clkkzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we need to standardize $\\vec{r}(t)$. Often when using PCA, data is first z-scored (so that each feature has zero-mean and unit-variance). This prevents the PCs from being trvially biased towards the features that happen to have a greater range.  However, this isn't really a desirable approach for neural data. This is because neurons with low firing rates have variance that is dominated by shot noise, so treating their variance equally with high-firing rate neurons will just add noise. Instead, we'll define a function ```normalize_spikes``` that preserves some, but not all, of the firing rate variance across the population. Then we'll apply this to  ```r_t```."],"metadata":{"id":"acuu8rO6lcQP"}},{"cell_type":"code","source":["def normalize_response(response,norm_factor=5):\n","  # transforms input data to have zero mean and variance that scales with its normalized range (range+norm_factor)\n","  # increasing norm_factor will make the output less sensitive to the data range\n","  # approach taken from: Russo et al. (2018). Neuron. \n","  # response must be of size ntimepoints x ncells\n","  norm_response=response.copy()\n","  for cell in range(0,ncells): # loop over cells\n","    this_response = response[:,cell]\n","    this_response = this_response - np.nanmean(this_response); # subtract mean\n","    ptp = np.nanmax(this_response) - np.nanmin(this_response)\n","    norm_response[:,cell] = this_response/(ptp+norm_factor); # normalize variance\n","  return norm_response\n","\n","r_t = normalize_response(r_t);"],"metadata":{"id":"8YFi3xINOTRY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Argh! One last bit of data wrangling.  Because the stimuli have different lengths, but it was desirable to make a uniform-sized array of responses, some trials have fewer than ```nbins``` elements and have been padded with NaNs. We'll define a variable ```missing_rows``` that indexes the timepoints (rows) of ```r_t``` that contain NaNs so we can remove them later on."],"metadata":{"id":"0JGef5MJr5-5"}},{"cell_type":"code","source":["missing_rows = np.isnan(r_t[:,0]);"],"metadata":{"id":"RB1Cd7tctTHq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 1a: Computing the PCs\n","\n","Phew! Now we can find the principal components $V$ of $\\vec{r}(t)$ (i.e. the variable ```r_t```).\n","\n","To jog your memory, recall that the principal components (PCs) are equivalent to the eigenvectors of $\\Sigma$, the covariance matrix of $\\vec{r}(t)$. The variance of the data projected on the PCs are the eigenvalues of $\\Sigma$.\n","\n","In other words, when we diagonalize $\\Sigma$ (i.e. define $V$ and $\\Lambda$ such that $\\Sigma = V \\Lambda V^T $), then the columns of $V$ are the eigenvectors (principal components) and the diagonal elements of the diagonal matrix $\\Lambda$ are the eigenvalues (principal component variances). (Note that the columns of $V$ and $\\Lambda$ are sorted so that the principal component variances are in descending order, by convention.) \n","\n","In the code block below, compute the full matrix of PCs $V$. This should be a square matrix of size ```ncells``` $\\times$ ```ncells```. (Remember to exclude the rows of the data indexed by ```missing_rows``` in your calculation. IOW, use ```r_t[~missing_rows,:]``` instead of ```r_t```).\n","\n","HINT: While you could find the answer to this and later questions by implementing the linear algebra in code, there are packages in Python that will do most of the work of PCA for you. We suggest checking out [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) from the scikit-learn package. (Although note that this package returns the principal components as the right eigenvectors of $\\Sigma$ (that is $V^T$ instead of $V$) \n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (3 pts)</font>"],"metadata":{"id":"7YomSxY2oTjD"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 1a\n","\n","# calculate PCs\n","# V = "],"metadata":{"id":"61et22Z44AaF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 1b: Principal Component Variances\n","\n","In the code block below, make a plot of the cumulative fraction of explained variance of $\\vec{r}(t)$ as a function of the number of PCs. \n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (2 pts)</font>"],"metadata":{"id":"k7sR8mTf2cvo"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 1b\n","\n","# plot cumulative fraction variance explained as a function of number of PCs\n"],"metadata":{"id":"vri-8I2c4KZ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What does the plot above look like? Roughly how many PCs do we need to explain 70% of the variance in the original data? How do you interpret this?\n","\n","<font color='red'>YOUR ANSWER IN TEXT HERE (2 pts)</font>"],"metadata":{"id":"ziIyxs_P04uJ"}},{"cell_type":"markdown","source":["### Question 1c: Computing the PC Scores\n","\n","Next, in the code block beloW:\n","- Compute the principal component scores  $\\vec{s}(t) = \\vec{r}(t)  V$. This is the transformation of the data in the new coordinate space defined by the principal components. (Again, remember to exclude the missing rows of the data indexed by ```missing_rows``` in your calculation.) Name the variable corresponding to the principal component scores ```s_t```.\n","- Then once you've obtained ```s_t```, reinsert missing values (using ```np.nan```) at the timepoints indexed by ```missing_rows``` and reshape the array to be the same size as the original ```ntrials``` $\\times$ ```nbins``` $\\times$ ```ncells``` array ```response```.\n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (2 pts)</font>"],"metadata":{"id":"mmFq1up22wam"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 1c\n","# calculate PC scores\n","\n","# initialize s_t with NaNs\n","s_t = np.empty(r_t.shape)\n","s_t[:]=np.nan\n","\n","# fill in non-missing rows with the transformed data\n","s_t[~missing_rows,:] = \n","\n","# reshape to be size of original \"response\" variable (ntrials x nbins x ncells)\n","s_t = "],"metadata":{"id":"0FctJeW64XcX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 1d: Visualizing the PC Scores\n","\n","Use the function ```plot_peth_scores_2d``` (defined in the code block below) to make a 2d plot illustrating the neural population trajectory in a space spanned by the first two PCs. Then repeat this with any other pair of PCs.\n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (2 pts)</font>"],"metadata":{"id":"QFmtJm0u4FZh"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 1d\n","\n","def plot_peth_scores_2d(s_t,PC1,PC2):\n","  # makes a 2d plot showing the trajectory of the population along two PCs, with trials broken down by evidence strength\n","  # s_t is an array of PC scores of size ntrials x nbins x npcs. \n","  # PC1 and PC2 are the one-based indices giving the desired pair of PCs to plot (i.e. lowest possible value is 1)\n","  # example: plot_peth_scores_2d(scores,1,2) will make a plot of the population trajectory across PCs 1 and 2\n","  peth,_,_ = get_peth(s_t[:,:,PC1-1],nboot=1)\n","  peth2,_,_ = get_peth(s_t[:,:,PC2-1],nboot=1)\n","  plt.gca().set_prop_cycle('color',colors)     \n","  plt.plot(np.transpose(peth),np.transpose(peth2),linewidth=5)    \n","  plt.xlabel(\"PC {pc:d}\".format(pc=PC1))\n","  plt.ylabel(\"PC {pc:d}\".format(pc=PC2))\n","  plt.legend(legend_str);    \n","\n","# plot PC 1 versus PC 2\n","\n","# plot some other PCs (use same axis range for better comparison)\n","\n"],"metadata":{"id":"rtAhwOxU4o8M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Describe what you observe in the plots you just made. What do you conclude about how the evolving decision process is represented in this population of neurons?\n","\n","<font color='red'>YOUR ANSWER IN TEXT HERE (2 pts)</font>"],"metadata":{"id":"6MHpU-JpwgSy"}},{"cell_type":"markdown","source":["## Question 2: Projecting Neural Activity on the \"Choice Axis\""],"metadata":{"id":"xcLhYEuzwKCm"}},{"cell_type":"markdown","source":["In Question 1, we performed PCA, a type of unsupervised dimensionality reduction. Now we'll perform **targeted dimensionality reduction**, where we seek to find a low-dimensional subspace that best aligns with some feature extrinsic to the neural data.\n","\n","Specifically, we'll use **logistic regression** to define an axis in neural state space that best predicts the animal's choices. In the neuroscience of decision making, this axis is sometimes called **\"the choice axis\"** and the projection of the population onto that axis **\"the decision variable\"** or \"DV\" for short.\n","\n","We'll define the vector $\\vec{\\beta}$ as the set of weights defining the choice axis in neural state space (see Fig. 2, left panel). $DV(t)$ is the projection of the neural population response on trial $t$ onto this axis: that is, $ DV(t) = \\vec{r}(t) \\vec{\\beta}  $. The goal is to find $\\vec{\\beta}$ that maximizes our ability to predict the animal's choices from $DV(t)$. (Since there is only one choice per trial, we'll use $t$ to index trials in Question 2, and we'll let $\\vec{r}(t)$ be the time-averaged response of the neural population on trial $t$). \n","\n","Because $DV(t)$ can take any real value while choices are binary, we'll model them as being related in the following way: $ p(R) = logistic(DV(t)) = \\frac{1}{1+e^{-DV(t)}} $. Under this model, the probability $p(R)$ of a rightward choice is a sigmoidal function of the decision variable (see Fig. 2, right panel). \n","\n","<figure>\n","<table><tr>\n","  <td bgcolor=\"#FFFFFF\"><img src=\"https://github.com/Brody-Lab/adrian_striatum_analysis/raw/master/images/logistic%20regression%201.png\" class=\"center\" alt=\"Schematic of Choice Axis in Neural State Space\" height=\"400\"></td>\n","<td bgcolor=\"#FFFFFF\">  <img src=\"https://github.com/Brody-Lab/adrian_striatum_analysis/raw/master/images/logistic%20regression%202.png\" class=\"center\" alt=\"Schematic of Choice Axis in Neural State Space\" height=\"370\"></td>\n","</tr></table>\n","\n","<figcaption><b>Figure 2.</b> <i>Left panel</i> Illustration of the choice axis, defined by vector $\\vec{\\beta}$, in neural state space. The projection of the average neural response on trial $t$ onto this axis defines the magnitude of the decision variable at that time point. The grey square indicates the plane defined by $DV=0$.  <i>Right panel</i> The probability of a rightward choice is a sigmoidal function of the decision variable. A nice feature of the logistic model is that, for $DV=\\alpha$, a rightward choice is $e^\\alpha$ times as likely than a leftward choice.</figcaption></figure>\n"],"metadata":{"id":"xNdPeuaZvI7_"}},{"cell_type":"markdown","source":["### Question 2a: Calculating the choice axis using principal components regression\n","\n","As we learned in problem 1, neural responses are largely confined to a low-dimensional manifold in neural state space. Therefore, it seems reasonable to expect that $\\vec{\\beta}$ will also live along that manifold. (In other words, it would be weird if the dimension in neural state that best predicted choice was one of the low-variance dimensions in the data). Therefore, instead of trying to estimate $\\vec{\\beta}$ in the traditional manner outlined above, we're actually going to estimate $\\vec{\\beta}_{PCR}$ such that $ DV(t) =   \\vec{s_L}(t)\\space \\vec{\\beta}_{PCR}$ where $\\vec{s_L}(t)$ are the $L$ PC scores corresponding to the top $L$ principal components of $\\vec{r}(t)$. This explicitly imposes the constraint that the choice axis must live in the subspace spanned by the top $L$ principal components of the data. This is referred to as principal components regression or PCR.\n","\n","One way to think about PCR is as a form of model shrinkage, akin to other methods you may have heard of (like ridge regression). Like those other methods, we are setting small coefficients to 0, under the assumption that small coefficients are likely capturing noise rather than signal. PCR provides a specific way to deciding which coefficients to set to 0, specifically those that correspond to low-variance principal components. \n","\n","As we mentioned above, for the purposes of Question 2, $t$ now indexes trials rather than time bins. In the code block below, take the scores from Question 1 (i.e. the variable ```s_t```) and average across timepoints, obtaining a single value for each trial and PC. Save this in a new variable called ```s_t_average``` of size ```ntrials``` $\\times$ ```ncells```. Ignore the first 0.2s of data from each trial, when the decision process is still early in its evolution and there is little information with which to decode choice. \n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (1 pt)</font>"],"metadata":{"id":"IkmouRRv1RDh"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 2a, part 1\n","\n","# remove first 0.2 s from each trial of PC scores and average over time bins (hint: use np.nanmean instead of np.mean to ignore missing time bins)\n","# s_t_average =  "],"metadata":{"id":"YK-k0MGR4-WY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now you'ready to perform (logistic) PCR using the time-averaged PC scores (the variable ```s_t_average```) as the set of predictors and the the variable ```went_right``` (i.e. the animal's choices) as the response. The first question we want to address is: how many PCs (columns of ```s_t_average```) should we use? This will tell us how small a subspace of the neural state space is sufficient for decoding choice.\n","\n","In the code block below:\n","- Fit a series of models from L = 1 to L = ```ncells```, each time using the first L columns of ```s_t_average``` as the predictors. Save the model accuracy (i.e. fraction of correctly predicted choices) for each of the L models.\n","- Define a variable ```best_L``` which is the value of L that leads to the best cross-validated model accuracy and save the vector of ```best_L``` estimated coefficients ($\\vec{\\beta}_{PCR}$) of that model as ```B_PCR```.\n","- Finally, plot the model accuracy as a function of L.\n","\n","It is important that you calculate the model accuracy on held-out data so that it reflects the model's ability to generalize, rather than its ability to fit every wiggle in the training data. To do this, we'll use ten-fold cross-validation.  For more about cross-validation and the problem of overfitting that it helps address, there are many resources on the web, like [this Nature article](https://www.nature.com/articles/nmeth.3968).\n","\n","There are many packages for doing logistic regression in Python, but we'll use [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) from the scikit-learn package. This will fit each of the L models in a single line (including the cross-validation step). You will need to click the link above and read over the documentation to be able to complete this question.\n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (4 pts)</font>"],"metadata":{"id":"-EhHFf8i7kAz"}},{"cell_type":"code","source":["# Note: Don't be surprised if this code cell takes a minute or two to execute!\n","###### YOUR CODE HERE #######\n","\n","# import LogisticRegressionCV and turn off annoying warnings\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.model_selection import KFold\n","from warnings import filterwarnings\n","filterwarnings('ignore') # suppresses the useless barage of warnings from LogisticRegressionCV\n","\n","# looping over number of pcs, calculate choice prediction accuracy (under k-fold cross-validation)\n","K = 10;\n","cv_obj = KFold(n_splits=K, shuffle=True, random_state=1)\n","model = LogisticRegressionCV(cv=cv_obj,Cs=[float('inf')],scoring='accuracy')\n","accuracy=np.empty(ncells)\n","best_accuracy=0\n","for L in range(ncells):\n","  # fit the model (initialized above) to the animal's choices using the first L PC scores averaged across trials (i.e. the first L columns of s_t_average)\n","  # YOUR CODE HERE, use LogisticRegressionCV's \"fit\" method\n","\n","  # then calculate the cross-validated accuracy of the fitted model (code provided)\n","  accuracy[L]=np.mean(model.scores_[1]) # take the average accuracy across the K cros-validation folds\n","\n","  # if the current value of accuracy is the best yet,\n","  # define best_L as the current number of PCs used\n","  # and define B_PCR as the current model coefficients\n","  if (YOUR LOGICAL STATEMENT HERE):\n","    best_accuracy=\n","    B_PCR = \n","    best_L =\n","\n","# plot choice prediction accuracy versus number of PCs\n","# YOUR CODE HERE"],"metadata":{"id":"Clw5dlSO5HIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What do you observe in the plot you just made? Does choice prediction accuracy increase monotonically as the number of PCs included in the model grows? If not, why not?\n","\n","<font color='red'>YOUR ANSWER IN TEXT (2 pts)</font>"],"metadata":{"id":"NEj5HgRm-xZP"}},{"cell_type":"markdown","source":["### Question 2b: Calculate and plot the \"decision variable\" trajectory on single trials\n","\n","Now, in the code block below:\n","- Compute $ DV(t) =   \\vec{s_L}(t)\\space \\vec{\\beta}_{PCR}$. In words, this means taking the best-fitting coefficients above as our estimate of the choice axis ($\\vec{\\beta}_{PCR}$) and project the ```best_L``` PC scores onto this axis to define a \"decision variable.\" Call the new variable containing this projection ```DV_t```. This variable should be of size ```ntrials``` $\\times$ ```nbins```. NOTE: since we averaged across time bins, ```s_t_average``` only contains a single timepoint per trial. But we want to visualize the evolution of the decision variable across time on single trials. So let's instead project the unaveraged PC scores (i.e. variable ```s_t``` from Question 1) onto $\\vec{\\beta}_{PCR}$ (or to be more precise those columns of ```s_t``` corresponding to the top ```best_L``` PCs). \n","- In a single plot, plot the trajectory of the decision variable on the first 20 right-choice trials and the first 20 left-choice trials. Use the variable ```time_s``` as the x-values and confine the x-axis range to ```time_range_s```. Use separate colors for the two trial types.\n","- Finally, pass the variable ```DV_t``` to the function ```plot_peth``` defined earlier to make a plot showing the average trajectory across trials separated by evidence strength.\n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (4 pts)</font>"],"metadata":{"id":"AUQtjfj6sovx"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 2b\n","\n","### Project the first L PC scores (i.e. \"s_t\" but including only the first L elements of its last dimension) onto the choice axis (i.e. \"B_PCR\") to get an estimate of the decision variable (DV_t)\n","### By \"project A onto B\" we mean \"take the dot product of A and B\"\n","B_PCR = np.squeeze(B_PCR) # np.squeeze removes singleton dimension to make B_PCR one-dimensional\n","DV_t =   # (Hint: use np.dot)\n","\n","# plot decision variable trajectory on first 20 left-choice trials and first 20 right-choice trials\n","#DV_t_left = YOUR CODE HERE # get values of DV_t, selecting only the trials where the animal went left\n","#DV_t_right = YOUR CODE HERE # get values of DV_t, selecting only the trials where the animal went right \n","plt.figure()\n","plt.plot(time_s,np.transpose(DV_t_right[:20,:]),'y-');\n","plt.plot(time_s,np.transpose(DV_t_left[:20,:]),'b-');\n","plt.xlim(time_range_s)\n","\n","# add x and y labels\n","# YOUR CODE HERE\n","\n","# plot trial-average decision variables broken down by momentary evidence strength (using function \"plot_peth\")\n","# YOUR CODE HERE\n"],"metadata":{"id":"mxZrhU5N6Wl2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What do you observe in the two plots above? Are there any trials in the first plot where the trajectory of DV seems to change sign? How does the evidence strength influence the way the decision variable evolves across time? Are your observations consistent with the idea that the decision variable gives us a readout of the ongoing decision process in the animal's brain? Why or why not?\n","\n","<font color='red'>YOUR ANSWER IN TEXT HERE (2 pts)</font>"],"metadata":{"id":"qYQbVNW05_f4"}},{"cell_type":"markdown","source":["## EXTRA CREDIT Question 3: Analyzing putative \"changes of mind\" on single trials"],"metadata":{"id":"X_nj63LewYgv"}},{"cell_type":"markdown","source":["Let's explore the cases where DV changes sign on single trials. These sign changes have been interpreted in past studies as putative **\"changes on mind\" (CoMs)**: that is, moments in time where the subject's provisional decision changes. If that is really true, it shouldn't occur randomly but instead should exhibit some statistical regularities. For one, we'd expect CoMs to occur less frequently as the trial progresses , since the subject will have had more time to develop confidence in their choice. By similar reasoning, we'd expect to see CoMs less often on trials with stronger evidence.\n","\n","In the code block below, determine whether or not the data support these predictions:\n","- First, define a variable ```CoMs``` that contains a 1 for time points in each trial when ```DV_t``` changes sign and a 0 otherwise.\n","- Make a plot showing the frequency of CoMs as a function of time across trials (i.e. plot the value of ```CoMs``` averaged over trials).\n","- Make a plot showing the per-timepoint frequency of CoMs as a function of evidence strength. To do this, group trials into three groups according to evidence strength (i.e. according to ```np.abs(evidence_strength)```) and then, within each such group, find the average value of ```CoMs``` across trials and timepoints.\n","\n","**NOTE**: Confine your CoM analysis to time bins in the range given by ```time_range_s``` (defined earlier). Timepoints earlier than this contain too many CoMs to be meaningful (since very little evidence has been presented yet) and timepoints after this include data from too few trials for sufficient statistical power.\n","\n","<font color='red'>YOUR ANSWER IN CODE BELOW (5 pts)</font>"],"metadata":{"id":"VsWTMbpS6YB7"}},{"cell_type":"code","source":["###### YOUR CODE HERE #######\n","\n","# Question 3\n","# Analyzing CoMs\n","\n","# index CoMs\n","# CoMs = \n","\n","# plot frequency of CoMs across time bins\n","\n","# plot frequency of CoMs by evidence strength\n"],"metadata":{"id":"ty8luMgw6n8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What do you observe in the plots above? Are these observations consistent with the predictions we made?\n","\n","<font color='red'>YOUR ANSWER IN TEXT (2 pts)</font>\n","\n","---\n","*fin*"],"metadata":{"id":"qdSb7cyHay1P"}}]}